from flask import Flask, jsonify, render_template
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from math import gcd
import json # jsonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèªã®ãŸã‚ã«æ®‹ã™

app = Flask(__name__)

# --- è¨­å®šé …ç›®ï¼ˆBASE_URLã¯è¨­å®šã•ã‚Œã¦ã„ãªã‹ã£ãŸãŸã‚ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼URLã‚’ä½¿ç”¨ï¼‰---
# ğŸŒ Webã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ«ãƒ¼ãƒˆURLã«åˆã‚ã›ã¦é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®š
BASE_URL_PLACEHOLDER = "/static"

TARGET_SITES = [
    {
        'type': 'element',
        'display_name': 'ãƒãƒ³ãƒãƒ—ãƒãƒ¼ã‚‚ãã‚‰',
        'name': 'mogura',
        'url': 'https://member.nonhapumogura.com/',
        'selector': '#count-num',
        'image_url': BASE_URL_PLACEHOLDER + '/images/mogura.png'
    },
    {
        'type': 'paging_bbs',
        'display_name': '440',
        'name': '440',
        'base_url': 'https://rara.jp/bar440/',
        'page_url_prefix': 'https://rara.jp/bar440/link',
        'start_page': 1,
        'max_page': 10,
        'step': 1,
        'date_selector': 'i.fa-solid.fa-clock',
        'date_format': '%Y/%m/%d',
        'image_url': BASE_URL_PLACEHOLDER + '/images/440.png'
    },
    {
        'type': 'paging_bbs_gender',
        'display_name': 'ã‚«ãƒãƒ­',
        'name': 'canelo',
        'base_url': 'https://barcanelo.com/bbs/index.php?page=0',
        'page_url_prefix': 'https://barcanelo.com/bbs/index.php?page=',
        'start_page': 0,
        'max_page': 10,
        'step': 10,
        'date_selector': 'span.date',
        'gender_selector': 'span.sex',
        'date_format': '%Y/%m/%d',
        'image_url': BASE_URL_PLACEHOLDER + '/images/canelo.png'
    },
    {
        'type': 'paging_bbs_gender',
        'display_name': 'ãƒªãƒˆãƒªãƒ¼ãƒˆãƒãƒ¼',
        'name': 'retreatbar',
        'base_url': 'https://retreatbar.jp/bbs/index.php?page=0',
        'page_url_prefix': 'https://retreatbar.jp/bbs/index.php?page=',
        'start_page': 0,
        'max_page': 10,
        'step': 10,
        'date_selector': 'span.date',
        'gender_selector': 'span.sex',
        'date_format': '%Y/%m/%d',
        'image_url': BASE_URL_PLACEHOLDER + '/images/retreatbar.png'
    },
    {
        'type': 'paging_bbs_gender',
        'display_name': 'ãƒãƒ¼ãƒ•ã‚§ã‚¤ã‚¹',
        'name': 'bar-face',
        'base_url': 'https://bar-face.jp/bbs/index.php?page=0',
        'page_url_prefix': 'https://bar-face.jp/bbs/index.php?page=',
        'start_page': 0,
        'max_page': 10,
        'step': 10,
        'date_selector': 'span.date',
        'gender_selector': 'span.sex',
        'date_format': '%Y/%m/%d',
        'image_url': BASE_URL_PLACEHOLDER + '/images/bar-face.png'
    },
]
# -----------------

# çŠ¶æ…‹ç®¡ç†ã®ãŸã‚ã®è¾æ›¸ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«I/Oã®ä»£ã‚ã‚Šã«ä½¿ç”¨ï¼‰
# æœ€å¾Œã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã¨å®Ÿè¡Œæ™‚åˆ»ã‚’ä¿å­˜
SCRAPED_DATA_CACHE = {
    'last_updated': None,
    'data': None
}

# --- ã‚¹ã‚¯ãƒ¬ãƒ”ãƒ³ã‚°é–¢æ•°ã®å®šç¾© ---

def get_post_count_from_element(site):
    """ å˜ä¸€ã®è¦ç´ ã‹ã‚‰ç›´æ¥æ›¸ãè¾¼ã¿æ•°ã‚’å–å¾—ã™ã‚‹ """
    print(f"Checking '{site['display_name']}'...")
    headers = {'User-Agent': 'MyScraper/1.0'}
    
    try:
        response = requests.get(site['url'], headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        count_element = soup.select_one(site['selector'])
        
        display_text = count_element.text.strip() if count_element else 'å–å¾—å¤±æ•—'
            
    except requests.exceptions.RequestException as e:
        print(f"  -> Error fetching {site['url']}: {e}")
        display_text = 'ã‚¨ãƒ©ãƒ¼'
    except Exception as e:
        print(f"  -> Unexpected error: {e}")
        display_text = 'å‡¦ç†ã‚¨ãƒ©ãƒ¼'

    return {
        'display_name': site['display_name'],
        'count': display_text,
        'url': site['url'],
        'type': 'simple',
        'image_url': site['image_url']
    }

def get_today_post_count_from_paging_site(site):
    """ ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã•ã‚ŒãŸæ²ç¤ºæ¿ã‚’å·¡å›ã—ã€ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã‚’é›†è¨ˆã™ã‚‹ """
    today_str = datetime.now().strftime(site['date_format'])
    today_post_count = 0
    headers = {'User-Agent': 'MyPagingScraper/1.0'}

    print(f"Checking '{site['display_name']}' (Date: {today_str})...")

    try:
        for page_num in range(site['start_page'], site['max_page'] + 1, site['step']):
            target_url = site['base_url'] if page_num == site['start_page'] else f"{site['page_url_prefix']}{page_num}"
            print(f"  -> page {page_num} ({target_url})")

            response = requests.get(target_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            post_times = soup.select(site['date_selector'])
            
            if not post_times:
                print("    -> No date info found. Stopping.")
                break

            is_today_post_found_on_page = False
            for time_element in post_times:
                post_datetime_str = ''
                if site['name'] == '440':
                    next_node = time_element.next_sibling
                    if next_node and next_node.string:
                        post_datetime_str = next_node.string.strip()
                else:
                    post_datetime_str = time_element.text.strip()
                
                if today_str in post_datetime_str:
                    today_post_count += 1
                    is_today_post_found_on_page = True
            
            if not is_today_post_found_on_page and page_num > site['start_page']:
                print("    -> No more posts for today. Stopping.")
                break
    except requests.exceptions.RequestException as e:
        print(f"    -> Error: {e}")
    except Exception as e:
        print(f"    -> Unexpected error: {e}")
    
    display_text = f"{today_post_count}ä»¶"
    return {
        'display_name': site['display_name'],
        'count': display_text,
        'url': site['base_url'],
        'type': 'simple',
        'image_url' : site['image_url']
    }

def get_today_post_count_with_gender(site):
    """ æ€§åˆ¥ã”ã¨ã«ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã‚’é›†è¨ˆã™ã‚‹ """
    today_str = datetime.now().strftime(site['date_format'])
    gender_count = {'ç”·æ€§': 0, 'å¥³æ€§': 0, 'ä¸æ˜': 0}
    headers = {'User-Agent': 'MyPagingScraper/1.0'}

    print(f"Checking '{site['display_name']}' with gender (Date: {today_str})...")

    try:
        for page_num in range(site['start_page'], site['max_page'] + 1, site['step']):
            target_url = site['base_url'] if page_num == site['start_page'] else f"{site['page_url_prefix']}{page_num}"
            print(f"  -> page {page_num} ({target_url})")

            response = requests.get(target_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            posts = soup.select('dl.contributor')
            
            if not posts:
                print("    -> No posts found. Stopping.")
                break

            is_today_post_found_on_page = False
            
            for post in posts:
                date_element = post.select_one(site['date_selector'])
                if not date_element:
                    continue
                    
                post_date_str = date_element.text.strip()
                
                if today_str in post_date_str:
                    is_today_post_found_on_page = True
                    
                    gender_element = post.select_one(site['gender_selector'])
                    if gender_element:
                        gender_text = gender_element.text.strip()
                        
                        if 'ç”·' in gender_text or 'male' in gender_text.lower():
                            gender_count['ç”·æ€§'] += 1
                        elif 'å¥³' in gender_text or 'female' in gender_text.lower():
                            gender_count['å¥³æ€§'] += 1
                        else:
                            gender_count['ä¸æ˜'] += 1
                    else:
                        gender_count['ä¸æ˜'] += 1
                
                # å¤ã„æ—¥ä»˜ã«é”ã—ãŸã‚‰åœæ­¢
                try:
                    if datetime.strptime(post_date_str, site['date_format']) < datetime.strptime(today_str, site['date_format']):
                         # åŒã˜ãƒšãƒ¼ã‚¸å†…ã«ä»Šæ—¥ã®æ—¥ä»˜ã®æŠ•ç¨¿ãŒæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€ãƒšãƒ¼ã‚¸å…¨ä½“ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                        break
                except ValueError:
                    # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
            
            if not is_today_post_found_on_page and page_num > site['start_page']:
                print("    -> No more posts for today. Stopping.")
                break
    except requests.exceptions.RequestException as e:
        print(f"    -> Error: {e}")
    except Exception as e:
        print(f"    -> Unexpected error: {e}")

    total = sum(gender_count.values())
    
    # ç”·å¥³æ¯”ã‚’è¨ˆç®—ï¼ˆæœ€å°ã®è‡ªç„¶æ•°æ¯”ï¼‰
    ratio = "è¨ˆç®—ä¸å¯"
    if gender_count['ç”·æ€§'] > 0 and gender_count['å¥³æ€§'] > 0:
        common_divisor = gcd(gender_count['ç”·æ€§'], gender_count['å¥³æ€§'])
        male_ratio = gender_count['ç”·æ€§'] // common_divisor
        female_ratio = gender_count['å¥³æ€§'] // common_divisor
        ratio = f"{male_ratio}:{female_ratio}"
    elif gender_count['ç”·æ€§'] > 0:
        ratio = "ç”·æ€§ã®ã¿"
    elif gender_count['å¥³æ€§'] > 0:
        ratio = "å¥³æ€§ã®ã¿"
    
    print(f"  -> Total: {total}, Male: {gender_count['ç”·æ€§']}, Female: {gender_count['å¥³æ€§']}, Unknown: {gender_count['ä¸æ˜']}, Ratio: {ratio}")
    
    return {
        'display_name': site['display_name'],
        'count': f"{total}ä»¶",
        'url': site['base_url'],
        'type': 'gender',
        'image_url' : site['image_url'],
        'gender_detail': {
            'male': gender_count['ç”·æ€§'],
            'female': gender_count['å¥³æ€§'],
            'unknown': gender_count['ä¸æ˜'],
            'ratio': ratio
        }
    }


def scrape_data(force_run=False):
    """ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€çµæœã®è¾æ›¸ã‚’è¿”ã™ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰ """
    
    global SCRAPED_DATA_CACHE
    cache = SCRAPED_DATA_CACHE
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ã‚’ãƒã‚§ãƒƒã‚¯ (1æ™‚é–“ = 3600ç§’)
    is_cache_stale = (cache['last_updated'] is None or 
                      (datetime.now() - cache['last_updated']).total_seconds() > 3600)
    
    if not force_run and not is_cache_stale:
        print("Returning data from cache.")
        return cache['data']

    print("Executing full scrape...")
    results = []

    for site in TARGET_SITES:
        try:
            if site['type'] == 'element':
                results.append(get_post_count_from_element(site))
            elif site['type'] == 'paging_bbs':
                results.append(get_today_post_count_from_paging_site(site))
            elif site['type'] == 'paging_bbs_gender':
                results.append(get_today_post_count_with_gender(site))
        except Exception as e:
            print(f"An unexpected error occurred for {site['display_name']}: {e}")
            results.append({
                'display_name': site['display_name'],
                'count': 'å‡¦ç†ã‚¨ãƒ©ãƒ¼',
                'url': site.get('url') or site.get('base_url', 'N/A'),
                'type': site['type'],
                'image_url': site['image_url']
            })

    final_data = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'post_data': results
    }
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
    SCRAPED_DATA_CACHE['last_updated'] = datetime.now()
    SCRAPED_DATA_CACHE['data'] = final_data
    
    return final_data

# --- Flask ãƒ«ãƒ¼ãƒˆå®šç¾© ---

@app.route('/')
def index():
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ï¼ˆindex.htmlï¼‰ã‚’è¡¨ç¤ºã™ã‚‹"""
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™
    return render_template('index.html')

@app.route('/api/posts')
def get_posts():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã€ã¾ãŸã¯æ–°è¦ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™API"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªã‚‰ãã‚Œã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ (force_run=False)
        data = scrape_data(force_run=False)
        return jsonify(data)
    except Exception as e:
        print(f"API Error in get_posts: {e}")
        return jsonify({'error': f'ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}'}), 500

@app.route('/api/refresh')
def force_refresh():
    """å¼·åˆ¶çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™API"""
    try:
        # å¼·åˆ¶çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ (force_run=True)
        data = scrape_data(force_run=True)
        return jsonify(data)
    except Exception as e:
        print(f"API Error in force_refresh: {e}")
        return jsonify({'error': f'å¼·åˆ¶æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}'}), 500

if __name__ == '__main__':
    # é–‹ç™ºç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹å ´åˆã®æ³¨æ„: 
    # Flaskã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®reloaderã¯ãƒ—ãƒ­ã‚»ã‚¹ã‚’å†èµ·å‹•ã™ã‚‹ãŸã‚ã€SCRAPED_DATA_CACHEã‚‚ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚
    app.run(debug=True, host='0.0.0.0')
